"""å°çº¢ä¹¦å†…å®¹å‘å¸ƒèŠ‚ç‚¹ - ä½œä¸º LangGraph ä¸­çš„ä¸€ä¸ªèŠ‚ç‚¹

è¿™ä¸ªèŠ‚ç‚¹å¯ä»¥è¢«ä¸Šå±‚ Agent/Supervisor è°ƒç”¨
"""

from typing import Annotated, Literal
from typing_extensions import TypedDict

from langchain_core.messages import BaseMessage, AIMessage, HumanMessage, SystemMessage
from langchain_core.tools import StructuredTool
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode
from pydantic import BaseModel, Field

from ...client import QwenClient
from ...tools.logging import get_logger
from ...tools.xhs.content_generator import _generate_content_workflow

logger = get_logger(__name__)


# ============================================================================
# 1. å®šä¹‰å·¥å…·
# ============================================================================

class ContentGenerationInput(BaseModel):
    """å†…å®¹ç”Ÿæˆå·¥å…·çš„è¾“å…¥å‚æ•°"""
    description: str = Field(
        description="å†…å®¹æè¿°æˆ–ä¸»é¢˜"
    )
    image_count: int = Field(
        default=3,
        ge=1,
        le=9,
        description="å›¾ç‰‡æ•°é‡ï¼ŒèŒƒå›´ 1-9"
    )


async def generate_xhs_content_tool(
    description: str,
    image_count: int = 3,
) -> str:
    """ç”Ÿæˆå¹¶å‘å¸ƒå°çº¢ä¹¦å†…å®¹"""
    try:
        logger.info(
            "XHS Node: Generating content",
            description=description[:50],
            image_count=image_count
        )
        
        result = await _generate_content_workflow(
            description=description,
            image_count=image_count,
            publish=True,
        )
        
        if result.get("success"):
            content = result.get("content", {})
            title = content.get("title", "")
            images_count = len(result.get("images", []))
            publish_result = result.get("publish", {})
            
            message = f"âœ… å°çº¢ä¹¦å†…å®¹å·²ç”Ÿæˆ\n"
            message += f"æ ‡é¢˜ï¼š{title}\n"
            message += f"å›¾ç‰‡ï¼š{images_count}å¼ \n"
            
            if publish_result and publish_result.get("success"):
                note_id = publish_result.get("note_id", "")
                message += f"çŠ¶æ€ï¼šå·²å‘å¸ƒ\n"
                if note_id:
                    message += f"ç¬”è®°IDï¼š{note_id}"
            
            return message
        else:
            return f"âŒ ç”Ÿæˆå¤±è´¥ï¼š{result.get('error', 'æœªçŸ¥é”™è¯¯')}"
            
    except Exception as e:
        logger.error("XHS Node: Tool execution failed", error=str(e))
        return f"âŒ æ‰§è¡Œå‡ºé”™ï¼š{str(e)}"


def create_xhs_tool() -> StructuredTool:
    """åˆ›å»ºå°çº¢ä¹¦ç”Ÿæˆå·¥å…·"""
    return StructuredTool.from_function(
        func=generate_xhs_content_tool,
        name="generate_xhs_content",
        description="ç”Ÿæˆå¹¶å‘å¸ƒå°çº¢ä¹¦å†…å®¹ã€‚æ ¹æ®æè¿°ç”Ÿæˆæ ‡é¢˜ã€æ­£æ–‡å’Œé…å›¾å¹¶å‘å¸ƒã€‚",
        args_schema=ContentGenerationInput,
        coroutine=generate_xhs_content_tool,
    )


# ============================================================================
# 2. å°çº¢ä¹¦ Agent èŠ‚ç‚¹
# ============================================================================

class XHSAgentNode:
    """å°çº¢ä¹¦å†…å®¹å‘å¸ƒ Agent èŠ‚ç‚¹
    
    å¯ä»¥ä½œä¸º LangGraph ä¸­çš„ä¸€ä¸ªèŠ‚ç‚¹è¢«è°ƒç”¨
    """
    
    def __init__(
        self,
        llm_model: str = "qwen-plus",
        llm_temperature: float = 0.7,
    ):
        """åˆå§‹åŒ–èŠ‚ç‚¹
        
        Args:
            llm_model: LLM æ¨¡åž‹åç§°
            llm_temperature: æ¸©åº¦å‚æ•°
        """
        self.logger = logger
        
        # åˆå§‹åŒ– LLM
        llm_client = QwenClient(
            model=llm_model,
            temperature=llm_temperature
        )
        self.llm = llm_client.client
        
        # åˆ›å»ºå·¥å…·
        self.tool = create_xhs_tool()
        self.tools = [self.tool]
        
        # ç»‘å®šå·¥å…·åˆ° LLM
        self.llm_with_tools = self.llm.bind_tools(self.tools)
        
        # ç³»ç»Ÿæç¤ºè¯
        self.system_prompt = SystemMessage(content="""ä½ æ˜¯å°çº¢ä¹¦å†…å®¹ç”Ÿæˆä¸“å®¶ã€‚

å½“æ”¶åˆ°ç”Ÿæˆå°çº¢ä¹¦å†…å®¹çš„è¯·æ±‚æ—¶ï¼š
1. åˆ†æžç”¨æˆ·çš„å†…å®¹éœ€æ±‚ï¼ˆä¸»é¢˜ã€å›¾ç‰‡æ•°é‡ç­‰ï¼‰
2. è°ƒç”¨ generate_xhs_content å·¥å…·ç”Ÿæˆå†…å®¹
3. è¿”å›žç”Ÿæˆç»“æžœ

é‡è¦ï¼š
- é»˜è®¤ç”Ÿæˆ 3 å¼ å›¾ç‰‡
- å¿…é¡»ä»Žç”¨æˆ·æ¶ˆæ¯ä¸­æå–å†…å®¹æè¿°
- å¦‚æžœä¿¡æ¯ä¸è¶³ï¼Œè¯´æ˜Žéœ€è¦æ›´å¤šä¿¡æ¯
""")
        
        self.logger.info(
            "XHS Agent Node initialized",
            model=llm_model,
        )
    
    async def __call__(self, state: dict) -> dict:
        """èŠ‚ç‚¹æ‰§è¡Œå‡½æ•°ï¼ˆä½œä¸º LangGraph èŠ‚ç‚¹è°ƒç”¨ï¼‰
        
        Args:
            state: å›¾çš„çŠ¶æ€ï¼Œå¿…é¡»åŒ…å« 'messages' å­—æ®µ
        
        Returns:
            æ›´æ–°åŽçš„çŠ¶æ€
        """
        messages = state["messages"]
        task_context = state.get("task_context")
        
        self.logger.info(
            "XHS Node: Processing request",
            message_count=len(messages)
        )
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤„ç†ä¸­
        if task_context:
            from ...core.models import TaskStatus
            task_context.mark_in_progress()
        
        # æ·»åŠ ç³»ç»Ÿæç¤ºè¯
        messages_with_system = [self.system_prompt] + list(messages)
        
        # è°ƒç”¨ LLM
        response = await self.llm_with_tools.ainvoke(messages_with_system)
        
        # å¦‚æžœæœ‰å·¥å…·è°ƒç”¨ï¼Œæ‰§è¡Œå·¥å…·
        if hasattr(response, "tool_calls") and response.tool_calls:
            self.logger.info(
                "XHS Node: Executing tools",
                tool_count=len(response.tool_calls)
            )
            
            result_dict = {}
            task_success = True
            error_message = None
            tool_result = None
            
            try:
                # æ‰§è¡Œå·¥å…·
                tool_node = ToolNode(self.tools)
                tool_result = await tool_node.ainvoke({
                    "messages": messages_with_system + [response]
                })
                
                # æ£€æŸ¥å·¥å…·æ‰§è¡Œç»“æžœæ˜¯å¦åŒ…å«é”™è¯¯
                tool_messages = tool_result.get("messages", [])
                if tool_messages:
                    last_tool_msg = tool_messages[-1]
                    if hasattr(last_tool_msg, "content"):
                        tool_result_content = last_tool_msg.content
                        # æ£€æŸ¥æ˜¯å¦åŒ…å«é”™è¯¯æ ‡è¯†
                        if isinstance(tool_result_content, str):
                            # æ£€æŸ¥å„ç§é”™è¯¯æ ‡è¯†
                            error_indicators = ["âŒ", "å¤±è´¥", "é”™è¯¯", "å‡ºé”™", "exception", "error", "failed"]
                            if any(indicator in tool_result_content.lower() for indicator in error_indicators):
                                task_success = False
                                error_message = tool_result_content
                                self.logger.warning(
                                    "XHS Node: Tool returned error result",
                                    error=error_message
                                )
                
                # å†æ¬¡è°ƒç”¨ LLM ç”Ÿæˆæœ€ç»ˆå“åº”
                final_messages = messages_with_system + [response] + tool_result["messages"]
                final_response = await self.llm.ainvoke(final_messages)
                
            except Exception as e:
                # ðŸ”¥ å…³é”®ä¿®å¤ï¼šæ•èŽ·å·¥å…·æ‰§è¡Œå¼‚å¸¸
                self.logger.error(
                    "XHS Node: Tool execution failed",
                    error=str(e),
                    exc_info=True
                )
                task_success = False
                error_message = str(e)
                
                # ç”Ÿæˆé”™è¯¯å“åº”æ¶ˆæ¯
                from langchain_core.messages import AIMessage
                final_response = AIMessage(
                    content=f"æŠ±æ­‰ï¼Œæ‰§è¡Œè¿‡ç¨‹ä¸­é‡åˆ°é”™è¯¯ï¼š{str(e)}"
                )
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            if task_context:
                from ...core.models import TaskStatus
                if task_success and tool_result:
                    # ä»»åŠ¡æˆåŠŸ
                    tool_messages = tool_result.get("messages", [])
                    result_info = {}
                    if tool_messages:
                        last_tool_msg = tool_messages[-1]
                        if hasattr(last_tool_msg, "content"):
                            result_info["tool_result"] = last_tool_msg.content
                    
                    task_context.mark_completed(result_info)
                    self.logger.info(
                        "XHS Node: Task completed and marked as COMPLETED",
                        task_id=task_context.task_id
                    )
                else:
                    # ä»»åŠ¡å¤±è´¥
                    task_context.mark_failed(error_message or "Unknown error")
                    self.logger.warning(
                        "XHS Node: Task failed and marked as FAILED",
                        task_id=task_context.task_id,
                        error=error_message
                    )
                
                result_dict["task_context"] = task_context
            
            self.logger.info(f"XHS Node: Task {'completed' if task_success else 'failed'}")
            return {
                "messages": [final_response],
                **result_dict
            }
        
        # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç›´æŽ¥è¿”å›žå“åº”
        # å¦‚æžœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œä¹Ÿæ ‡è®°ä¸ºå·²å®Œæˆï¼ˆå¯èƒ½æ˜¯ç®€å•æŸ¥è¯¢ï¼‰
        if task_context:
            from ...core.models import TaskStatus
            task_context.mark_completed({"message": "No tool execution needed"})
            self.logger.info("XHS Node: Responded without tool execution, task marked as completed")
            return {
                "messages": [response],
                "task_context": task_context
            }
        
        self.logger.info("XHS Node: Responded without tool execution")
        return {"messages": [response]}


# ============================================================================
# 3. ä¾¿æ·åˆ›å»ºå‡½æ•°
# ============================================================================

def create_xhs_agent_node(
    llm_model: str = "qwen-plus",
    llm_temperature: float = 0.7,
) -> XHSAgentNode:
    """åˆ›å»ºå°çº¢ä¹¦ Agent èŠ‚ç‚¹
    
    Args:
        llm_model: LLM æ¨¡åž‹
        llm_temperature: æ¸©åº¦å‚æ•°
    
    Returns:
        å¯è°ƒç”¨çš„èŠ‚ç‚¹å®žä¾‹
    
    Example:
        >>> xhs_node = create_xhs_agent_node()
        >>> # åœ¨ä¸Šå±‚å›¾ä¸­ä½¿ç”¨
        >>> workflow.add_node("xhs_agent", xhs_node)
    """
    return XHSAgentNode(
        llm_model=llm_model,
        llm_temperature=llm_temperature,
    )