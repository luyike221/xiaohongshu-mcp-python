import torch
from modelscope import ZImagePipeline
import gc
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import Optional
import io
import os
from datetime import datetime
from contextlib import asynccontextmanager

# ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„
MODEL_PATH = "/data/mxpt/models/Tongyi-MAI/Z-Image-Turbo"

# è¾“å‡ºç›®å½•
OUTPUT_DIR = "output"

# å…¨å±€å˜é‡å­˜å‚¨æ¨¡å‹ç®¡é“
pipe = None


@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼šå¯åŠ¨æ—¶åŠ è½½æ¨¡å‹ï¼Œå…³é—­æ—¶æ¸…ç†"""
    global pipe
    
    # å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹
    print("ğŸ”„ å¼€å§‹åŠ è½½æ¨¡å‹ï¼ˆå†…å­˜ä¼˜åŒ–æ¨¡å¼ï¼‰...")
    print("ğŸ’¾ ä½¿ç”¨ CPU offloading å’Œä½å†…å­˜æ¨¡å¼")
    
    # æ¸…ç†å†…å­˜
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    # åŠ è½½æœ¬åœ°æ¨¡å‹ - å†…å­˜ä¼˜åŒ–é…ç½®
    try:
        pipe = ZImagePipeline.from_pretrained(
            MODEL_PATH,
            torch_dtype=torch.bfloat16,
            low_cpu_mem_usage=True,
        )
        print("âœ… ä½¿ç”¨ bfloat16 ç²¾åº¦")
    except Exception as e:
        print(f"âš ï¸ bfloat16 åŠ è½½å¤±è´¥ï¼Œå°è¯• float16: {e}")
        pipe = ZImagePipeline.from_pretrained(
            MODEL_PATH,
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True,
        )
        print("âœ… ä½¿ç”¨ float16 ç²¾åº¦")
    
    # å¯ç”¨ CPU offloading
    print("ğŸ”„ å¯ç”¨ CPU offloading...")
    pipe.enable_model_cpu_offload()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    print(f"ğŸ“ è¾“å‡ºç›®å½•å·²åˆ›å»º: {OUTPUT_DIR}")
    
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")
    
    yield
    
    # å…³é—­æ—¶æ¸…ç†
    print("ğŸ”„ æ¸…ç†æ¨¡å‹èµ„æº...")
    del pipe
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    print("âœ… èµ„æºæ¸…ç†å®Œæˆ")


app = FastAPI(lifespan=lifespan)


def adjust_to_multiple_of_16(value: int) -> int:
    """å°†æ•°å€¼è°ƒæ•´ä¸º16çš„å€æ•°ï¼ˆå‘ä¸‹å–æ•´ï¼‰"""
    return (value // 16) * 16


class GenerateRequest(BaseModel):
    """å›¾åƒç”Ÿæˆè¯·æ±‚æ¨¡å‹"""
    prompt: str
    height: Optional[int] = 1024
    width: Optional[int] = 1024
    num_inference_steps: Optional[int] = 9
    guidance_scale: Optional[float] = 0.0
    seed: Optional[int] = None


@app.post("/generate")
async def generate_image(request: GenerateRequest):
    """
    ç”Ÿæˆå›¾åƒæ¥å£
    
    - **prompt**: å›¾åƒæè¿°æç¤ºè¯ï¼ˆå¿…éœ€ï¼‰
    - **height**: å›¾åƒé«˜åº¦ï¼Œé»˜è®¤ 1024
    - **width**: å›¾åƒå®½åº¦ï¼Œé»˜è®¤ 1024
    - **num_inference_steps**: æ¨ç†æ­¥æ•°ï¼Œé»˜è®¤ 9
    - **guidance_scale**: å¼•å¯¼å¼ºåº¦ï¼Œé»˜è®¤ 0.0ï¼ˆTurbo æ¨¡å‹åº”ä¸º 0ï¼‰
    - **seed**: éšæœºç§å­ï¼Œé»˜è®¤ Noneï¼ˆéšæœºï¼‰
    """
    global pipe
    
    if pipe is None:
        return {"error": "æ¨¡å‹æœªåŠ è½½"}
    
    try:
        # è°ƒæ•´é«˜åº¦å’Œå®½åº¦ä¸º16çš„å€æ•°ï¼ˆæ¨¡å‹è¦æ±‚ï¼‰
        adjusted_height = adjust_to_multiple_of_16(request.height)
        adjusted_width = adjust_to_multiple_of_16(request.width)
        
        if adjusted_height != request.height or adjusted_width != request.width:
            print(f"âš ï¸ å°ºå¯¸å·²è‡ªåŠ¨è°ƒæ•´: {request.height}x{request.width} -> {adjusted_height}x{adjusted_width} (å¿…é¡»æ˜¯16çš„å€æ•°)")
        
        # è®¾ç½®éšæœºç§å­
        if request.seed is not None:
            generator = torch.Generator("cuda" if torch.cuda.is_available() else "cpu").manual_seed(request.seed)
        else:
            generator = None
        
        # ç”Ÿæˆå›¾åƒ
        print(f"ğŸ¨ å¼€å§‹ç”Ÿæˆå›¾åƒ: {request.prompt[:50]}...")
        result = pipe(
            prompt=request.prompt,
            height=adjusted_height,
            width=adjusted_width,
            num_inference_steps=request.num_inference_steps,
            guidance_scale=request.guidance_scale,
            generator=generator,
        )
        
        image = result.images[0]
        
        # ç”Ÿæˆæ–‡ä»¶åï¼ˆä½¿ç”¨æ—¶é—´æˆ³ï¼‰
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
        filename = f"generated_{timestamp}.png"
        filepath = os.path.join(OUTPUT_DIR, filename)
        
        # ä¿å­˜å›¾åƒåˆ° output ç›®å½•
        image.save(filepath, format='PNG')
        print(f"ğŸ’¾ å›¾åƒå·²ä¿å­˜åˆ°: {filepath}")
        
        # å°†å›¾åƒè½¬æ¢ä¸ºå­—èŠ‚æµç”¨äºè¿”å›
        img_byte_arr = io.BytesIO()
        image.save(img_byte_arr, format='PNG')
        img_byte_arr.seek(0)
        
        print("âœ… å›¾åƒç”Ÿæˆå®Œæˆ")
        
        return StreamingResponse(
            img_byte_arr,
            media_type="image/png",
            headers={"Content-Disposition": f"attachment; filename={filename}"}
        )
    
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå›¾åƒæ—¶å‡ºé”™: {e}")
        return {"error": str(e)}
